{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9009124b4c18f0eda044abc5e2ff141f",
     "grade": false,
     "grade_id": "cell-20cd3bb8f5a12f07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Actividad 2: Structured Streaming y Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Es muy importante eliminar siempre las líneas que contienen el código `raise NotImplementedError` ❌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1fed64c5c94f8b4c27326b8b48ec13f",
     "grade": false,
     "grade_id": "cell-2fbae7fd82212064",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Iniciamos Actividad 2 desde el final de la actividad 1: Copiamos función `retrasoMedioLlegada` de la Actividad 1 para la actividad 2.\n",
    "\n",
    "De los trayectos de llegada positiva del bus, calcular el retraso medio de llegada de cada estación.\n",
    "\n",
    "Importante, se debe de copiar la función de la actividad 1 llamada `retrasoMedioLlegada` en la siguiente celda para esta actividad 2. Lo que devuelve esta función debería de ser 2 columnas: `destination` y `retraso_medio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creando topic 'retrasos': KafkaError{code=TOPIC_ALREADY_EXISTS,val=36,str=\"Topic 'retrasos' already exists.\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'retrasos': TopicMetadata(retrasos, 1 partitions)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "bootstrap_servers = \"kafka:9092\"\n",
    "kafka_admin = AdminClient({\"bootstrap.servers\": bootstrap_servers})\n",
    "\n",
    "topic_name = \"retrasos\"\n",
    "retrasos_topic = NewTopic(topic_name, num_partitions=1, replication_factor=1)\n",
    "\n",
    "fs = kafka_admin.create_topics([retrasos_topic])\n",
    "\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()\n",
    "        print(f\"Topic '{topic}' creado correctamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creando topic '{topic}': {e}\")\n",
    "\n",
    "kafka_admin.list_topics().topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "bootstrap_servers = \"kafka:9092\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def retrasoMedioLlegada(df):\n",
    "    return (df.where(F.col(\"delay_end_M\") > 0)\n",
    "                .groupBy(\"destination\")\n",
    "                .agg(F.avg(\"delay_end_M\").alias(\"retraso_medio_llegada\"))\n",
    "                .orderBy(F.desc(\"retraso_medio_llegada\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Para implementar una solución en tiempo real utilizando Kafka de manera que nos aseguremos que los cálculos hechos anteriormente \"Actividad 1\" se actualicen continuamente y en tiempo real, es esencial la configuración adecuada de un DataFrame de Streaming en Spark. Aquí, nuestro objetivo es procesar eficientemente los datos provenientes de Kafka, enfocándonos en los campos `destination` y `delay_end_M`. El proceso implica la utilización de un Streaming DataFrame. El Dataframe será almacenado en la variable `delayStreamingDF` y será el encargado de leer datos del topic `retrasos` directamente desde Apache Kafka. Este Dataframe se construirá de la siguiente manera:\n",
    "\n",
    "**1. Uso de `readStream` en lugar de `read`:**\n",
    "\n",
    "- En nuestro entorno de Spark, en lugar de utilizar el método estándar read, optaremos por readStream. Esta elección es fundamental para establecer un flujo de datos continuo, permitiendo que nuestro DataFrame procese datos en tiempo real. readStream es una característica clave de Spark que habilita el procesamiento de streaming, esencial para trabajar con fuentes de datos en tiempo real como Kafka.\n",
    "    \n",
    "**2. Establecer el formato de origen a Kafka:**\n",
    "\n",
    "- Con el uso de .format(\"kafka\"), estamos instruyendo a Spark para que interprete los datos entrantes como provenientes de un servidor Kafka. Este paso es crucial porque define la manera en que Spark va a leer y decodificar los datos, asegurando la compatibilidad y la correcta interpretación del flujo de datos desde Kafka.\n",
    "    \n",
    "**3. Configuración de los brokers de Kafka y el puerto:**\n",
    "\n",
    "- Mediante la opción .option(\"kafka.bootstrap.servers\", \"<nombre-cluster>-w-0:9092,<nombre-cluster>-w-1:9092\"), especificamos los nodos de Kafka desde los cuales vamos a leer los datos. Aquí, <nombre-cluster>-w-0:9092,<nombre-cluster>-w-1:9092 se refiere a los brokers (nodos) de Kafka en nuestro clúster, con el puerto 9092, que es el puerto por defecto de Kafka. Este paso es vital para establecer una conexión efectiva con Kafka, permitiendo que nuestro DataFrame acceda a los datos distribuidos a través de múltiples nodos en el clúster.\n",
    "\n",
    "\n",
    "**4. Suscripción al tópico `retrasos`:**\n",
    "\n",
    "- Utilizando .option(\"subscribe\", \"retrasos\"), definimos explícitamente el tópico de Kafka al que nuestro DataFrame debe suscribirse. Al especificar \"retrasos\", nos aseguramos de que solo se lea y procese la información relevante para nuestra tarea, filtrando cualquier otro flujo de datos que no sea pertinente. Esta suscripción es esencial para dirigir el flujo de datos hacia los mensajes específicos que necesitamos.\n",
    "\n",
    "\n",
    "**5. Cargar la configuración con load():**\n",
    "\n",
    "- El último paso es invocar load(), lo cual inicia el proceso de lectura y procesamiento de los datos conforme a las configuraciones previas. Este comando es el que activa todas las opciones establecidas anteriormente, y es el paso final para que nuestro DataFrame de streaming comience a funcionar y procesar los datos transmitidos desde Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca3d9ff34a83745e199a3c4aed9acf07",
     "grade": false,
     "grade_id": "read-stream",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "delayStreamingDF = None # Sustituye con el código adecuado conforme a las instrucciones previas\n",
    "\n",
    "# Tu código hazlo aquí\n",
    "delayStreamingDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", bootstrap_servers).option(\"subscribe\", \"retrasos\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb5c34086b3c3fb29a4c46865396bcb1",
     "grade": true,
     "grade_id": "read-stream-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtenemos y almacenamos los tipos (columnTypes) de datos de cada columna en 'delayStreamingDF'\n",
    "columnTypes = delayStreamingDF.dtypes\n",
    "\n",
    "# Comprobamos que 'delayStreamingDF' es un DataFrame de streaming.\n",
    "assert(delayStreamingDF.isStreaming)\n",
    "\n",
    "# Verificamos que los tipos de datos de cada columna sean los esperados.\n",
    "assert((columnTypes[0][0] == \"key\")             & (columnTypes[0][1] == \"binary\"))\n",
    "assert((columnTypes[1][0] == \"value\")           & (columnTypes[1][1] == \"binary\"))\n",
    "assert((columnTypes[2][0] == \"topic\")           & (columnTypes[2][1] == \"string\"))\n",
    "assert((columnTypes[3][0] == \"partition\")       & (columnTypes[3][1] == \"int\"))\n",
    "assert((columnTypes[4][0] == \"offset\")          & (columnTypes[4][1] == \"bigint\"))\n",
    "assert((columnTypes[5][0] == \"timestamp\")       & (columnTypes[5][1] == \"timestamp\"))\n",
    "assert((columnTypes[6][0] == \"timestampType\")   & (columnTypes[6][1] == \"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "699b1979df4eb0241b1b4ba165d6b311",
     "grade": false,
     "grade_id": "cell-580bf3caf39b314e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "\n",
    "\n",
    "**1. Transformación de la Columna `value`:**\n",
    "\n",
    "- Selecciona la columna `value` y convierte `value` a `StringType` usando `withColumn`. Esto reemplazará la columna `value` existente con la nueva versión convertida. Este paso es crucial ya que permitirá manejar cada fila de esta columna como un fichero JSON completo, facilitando la extracción de datos específicos más adelante.\n",
    "    \n",
    "**2. Extracción de Datos de los JSON:**\n",
    "\n",
    "- Para obtener los campos individuales de cada JSON, usaremos la función `from_json` de Spark. Aplicaremos esta función a cada fila de la columna `value`, lo que nos permitirá parsear el String de JSON según un esquema definido. El resultado será una nueva columna `jsonData` de tipo `struct`, que incluye dos campos, uno de tipo `String` y otro de tipo `Integer`.\n",
    "    \n",
    "**3. Acceso a Campos de la Estructura `jsonData`:**\n",
    "\n",
    "- La columna `jsonData`, al ser un tipo `struct`, nos permite acceder a sus campos mediante el operador punto (.). Utilizando `withColumn` dos veces, crearemos dos nuevas columnas: `destination` y `delay_end_M`. Cada una de estas será el resultado de acceder a los campos respectivos de `jsonData`, es decir, `jsonData.destination` y `jsonData.delay_end_M`.\n",
    "    \n",
    "    \n",
    "Con estos pasos, convertiremos datos en bruto en una forma estructurada y utilizable, preparando el camino para análisis y operaciones más avanzadas.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91b265facf6e3bded458cc9b6a754b1a",
     "grade": false,
     "grade_id": "estructura-json",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "esquema = StructType([\\\n",
    "  StructField(\"destination\", StringType()),\\\n",
    "  StructField(\"delay_end_M\", DoubleType())\\\n",
    "])\n",
    "\n",
    "destinationDelaysDF = None   # Elimina y descomenta las siguientes lineas y verás que debes completar el código \" <COMPLETAR> \"\n",
    "\n",
    "destinationDelaysDF = delayStreamingDF\\\n",
    "    .select(\"value\")\\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(StringType()))\\\n",
    "    .withColumn(\"jsonData\", F.from_json(F.col(\"value\"), esquema))\\\n",
    "    .withColumn(\"destination\", F.col(\"jsonData.destination\"))\\\n",
    "    .withColumn(\"delay_end_M\", F.col(\"jsonData.delay_end_M\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fc63420d404c6ca1fedec52d4ee3b7a",
     "grade": true,
     "grade_id": "estructura-json-tests",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Almacena en 'columnTypes' los tipos (columnTypes) de datos de cada columna del DataFrame 'destinationDelaysDF'.\n",
    "columnTypes = destinationDelaysDF.dtypes\n",
    "assert((\"value\", \"string\") in columnTypes)\n",
    "assert(('jsonData', 'struct<destination:string,delay_end_M:double>') in columnTypes)\n",
    "assert(('destination', 'string') in columnTypes)\n",
    "assert(('delay_end_M', 'double') in columnTypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que nuestro DataFrame incluye una columna destination, que identifica la estación de destino, y una columna delay_end_M, que registra los retrasos en formato de números reales, estamos en condiciones de realizar un tipo de análisis similar al que se lleva a cabo en la función retrasoMedioLlegadaStreamingDF. Por lo tanto, procederemos a aplicar retrasoMedioLlegadaStreamingDF, utilizando destinationDelaysDF como su argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name retrasosAgg as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m      4\u001b[0m retrasoMedioLlegadaStreamingDF \u001b[38;5;241m=\u001b[39m retrasoMedioLlegada(destinationDelaysDF)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Configuramos y arrancamos la ejecución en streaming.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# .writeStream configura el flujo de escritura.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .queryName asigna un nombre a la consulta en streaming para su referencia futura.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# .outputMode establece el modo de salida 'complete', actualizando toda la tabla con cada disparador.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# .format especifica el formato de los resultados de la salida, en este caso, en memoria.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# .start inicia la ejecución de la consulta en streaming.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m consoleOutput \u001b[38;5;241m=\u001b[39m \u001b[43mretrasoMedioLlegadaStreamingDF\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrasosAgg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 17\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name retrasosAgg as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "# No modifiques el la celda de código\n",
    "# Se crea el DataFrame 'retrasoMedioLlegadaStreamingDF' mediante la función 'retrasoMedioLlegada'\n",
    "# sobre 'destinationDelaysDF', con el objetivo de calcular el promedio del tiempo de retraso en las llegadas a destino.\n",
    "retrasoMedioLlegadaStreamingDF = retrasoMedioLlegada(destinationDelaysDF)\n",
    "\n",
    "# Configuramos y arrancamos la ejecución en streaming.\n",
    "# .writeStream configura el flujo de escritura.\n",
    "# .queryName asigna un nombre a la consulta en streaming para su referencia futura.\n",
    "# .outputMode establece el modo de salida 'complete', actualizando toda la tabla con cada disparador.\n",
    "# .format especifica el formato de los resultados de la salida, en este caso, en memoria.\n",
    "# .start inicia la ejecución de la consulta en streaming.\n",
    "consoleOutput = retrasoMedioLlegadaStreamingDF\\\n",
    "                    .writeStream\\\n",
    "                    .queryName(\"retrasosAgg\")\\\n",
    "                    .outputMode(\"complete\")\\\n",
    "                    .format(\"memory\")\\\n",
    "                    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "018ce42cfbc94eef069b4a60e5e89090",
     "grade": false,
     "grade_id": "cell-b073802f3eec8bb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Después de completar la ejecución de la celda previa, inicia sesión en el productor de Kafka a través de SSH en una de las máquinas (puedes revisar las instrucciones de la práctica para recordar cómo hacerlo). Luego, inserta exactamente estos 4 mensajes a Kafka en formato JSON. Observarás que incluyen un campo 'destination' y un campo 'delay_end_M', representando los datos que recibiríamos en vivo de diferentes estaciones de bus conforme los trayectos van llegando a destino.\n",
    "\n",
    "Cada vez que introduzcas un mensaje, realiza una consulta utilizando `select * from retrasosAgg` mediante `spark.sql(...)` y observa los resultados en el DataFrame `retrasosAggregadosDF `. Esto ejecutará una consulta contra la vista temporal 'retrasosAgg' que se creó en el metastore de Hive gracias al `writeStream` del paso anterior. Continúa ejecutando el comando `show()` en esa celda hasta que observes un cambio en los resultados, lo cual confirmará que Spark ha procesado el nuevo dato en su cálculo de agregación y actualizado el resultado.\n",
    "\n",
    "Ten en cuenta que `spark.sql(...)` es una transformación, así que la consulta se ejecutará nuevamente cada vez que utilices `show()` en el resultado. No cachearás nada, lo cual es intencional para forzar la reevaluación de la consulta y así poder ver el contenido actualizado de la tabla (en memoria) en Hive cada vez que hagas `show()`.\n",
    "\n",
    "Lo que se solicita es:\n",
    "\n",
    "- Anota el resultado de la agregación (el valor de la columna 'retraso_medio_llegada') para SAO PAULO y RIO DE JANEIRO en las variables designadas para ello cada vez que envíes un mensaje y confirmes que Spark ha añadido esa información a su cálculo.\n",
    "- No te preocupes si necesitas ejecutar la misma celda varias veces. El cálculo solo se actualizará una vez con cada nuevo mensaje enviado a Kafka. Las ejecuciones subsiguientes mostrarán el mismo resultado hasta que envíes otro mensaje nuevo.\n",
    "\n",
    "Los 4 mensajes en formato JSON que debes ingresar sucesivamente en Kafka son los siguientes:\n",
    "\n",
    "- {\"destination\": \"SAO PAULO\", \"delay_end_M\": 4.1}\n",
    "- {\"destination\": \"RIO DE JANEIRO\", \"delay_end_M\": 8.6}\n",
    "- {\"destination\": \"SAO PAULO\", \"delay_end_M\": 2.1}\n",
    "- {\"destination\": \"RIO DE JANEIRO\", \"delay_end_M\": 42.2}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aca37e32dce02202f512568edca47c19",
     "grade": false,
     "grade_id": "sql-streaming",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n",
      "|destination|retraso_medio_llegada|\n",
      "+-----------+---------------------+\n",
      "+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrasosAggregadosDF = spark.sql(\"select * from retrasosAgg\")   # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "\n",
    "# Después de enviar el primer mensaje a Kafka que contiene datos para 'SAO PAULO'\n",
    "# Ejecutamos la consulta y mostramos los resultados.\n",
    "retrasosAggregadosDF.show()\n",
    "\n",
    "# Tu código hazlo aquí\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c17f79058004e6c56c6cb64795ae6f2",
     "grade": true,
     "grade_id": "sql-streaming-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columnas = retrasosAggregadosDF .columns\n",
    "assert(len(columnas) == 2)\n",
    "assert(\"destination\" in columnas)\n",
    "assert(\"retraso_medio_llegada\" in columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "709560e7fbed0009d74b7fec8c90a3c1",
     "grade": false,
     "grade_id": "results-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n",
      "|destination|retraso_medio_llegada|\n",
      "+-----------+---------------------+\n",
      "|  SAO PAULO|                  4.1|\n",
      "+-----------+---------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrasosAggregadosDF.show() # Muestra el DataFrame una vez Spark Streaming ya procesado la actualización (enviado a Kafka).\n",
    "result = spark.sql(\"select retraso_medio_llegada from retrasosAgg where destination = 'SAO PAULO'\")\n",
    "\n",
    "retraso_medio_SAO_PAULO_primer_mensaje = result.head()[\"retraso_medio_llegada\"]\n",
    "\n",
    "# Tu código hazlo aquí\n",
    "retraso_medio_SAO_PAULO_primer_mensaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b8f02459cd58afae6cf1a32ef69f36e",
     "grade": false,
     "grade_id": "results-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "|   destination|retraso_medio_llegada|\n",
      "+--------------+---------------------+\n",
      "|RIO DE JANEIRO|                  8.6|\n",
      "|     SAO PAULO|                  4.1|\n",
      "+--------------+---------------------+\n",
      "\n",
      "4.1\n",
      "8.6\n"
     ]
    }
   ],
   "source": [
    "retrasosAggregadosDF.show()\n",
    "\n",
    "result = spark.sql(\"select retraso_medio_llegada from retrasosAgg order by destination\").collect()\n",
    "\n",
    "retraso_medio_SAO_PAULO_segundo_mensaje = result[1][0]  # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "retraso_medio_RIO_JANEIRO_segundo_mensaje = result[0][0]  # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "\n",
    "# Tu código hazlo aquí\n",
    "print(retraso_medio_SAO_PAULO_segundo_mensaje)\n",
    "print(retraso_medio_RIO_JANEIRO_segundo_mensaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ed1d482d7859eaf314d85f7f056d79e",
     "grade": false,
     "grade_id": "results-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "|   destination|retraso_medio_llegada|\n",
      "+--------------+---------------------+\n",
      "|RIO DE JANEIRO|                  8.6|\n",
      "|     SAO PAULO|   3.0999999999999996|\n",
      "+--------------+---------------------+\n",
      "\n",
      "3.0999999999999996\n",
      "8.6\n"
     ]
    }
   ],
   "source": [
    "retrasosAggregadosDF.show()\n",
    "\n",
    "result = spark.sql(\"select retraso_medio_llegada from retrasosAgg order by destination\").collect()\n",
    "\n",
    "retraso_medio_SAO_PAULO_tercer_mensaje = result[1][0]  # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "retraso_medio_RIO_JANEIRO_tercer_mensaje = result[0][0]  # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "\n",
    "# Tu código hazlo aquí\n",
    "print(retraso_medio_SAO_PAULO_tercer_mensaje)\n",
    "print(retraso_medio_RIO_JANEIRO_tercer_mensaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e49b6258fd677f636bbc9f01e41a593",
     "grade": false,
     "grade_id": "results-4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "|   destination|retraso_medio_llegada|\n",
      "+--------------+---------------------+\n",
      "|RIO DE JANEIRO|   25.400000000000002|\n",
      "|     SAO PAULO|   3.0999999999999996|\n",
      "+--------------+---------------------+\n",
      "\n",
      "3.0999999999999996\n",
      "25.400000000000002\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta varias veces esta celda tras enviar el cuarto mensaje, hasta ver que el DataFrame ha cambiado\n",
    "retrasosAggregadosDF.show()\n",
    "\n",
    "result = spark.sql(\"select retraso_medio_llegada from retrasosAgg order by destination\").collect()\n",
    "\n",
    "retraso_medio_SAO_PAULO_cuarto_mensaje = result[1][0]  # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "retraso_medio_RIO_JANEIRO_cuarto_mensaje = result[0][0]  # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "\n",
    "# Tu código hazlo aquí\n",
    "print(retraso_medio_SAO_PAULO_cuarto_mensaje)\n",
    "print(retraso_medio_RIO_JANEIRO_cuarto_mensaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "**Procesamiento de Datos CSV en Tiempo Real con Spark y Kafka**\n",
    "\n",
    "El ejercicio 3 es parecido al ejercicio 2 pero en vez de utilizar el formato típico JSON se va a recibir a través de formato CSV. \n",
    "\n",
    "Objetivo: Procesar en tiempo real datos de estaciones de autobuses enviados a Kafka en formato CSV. Cada mensaje incluirá un `id_estacion`, `nombre_estacion`, y `numero_pasajeros`.\n",
    "\n",
    "Lo que se solicita: \n",
    "\n",
    "- Calcular el número total de pasajeros por estación.\n",
    "- Mostrar la ejecución de la celda, mostrará una tabla con el resultado de los 4 mensajes.\n",
    "\n",
    "Los 4 mensajes en formato CSV que debes ingresar sucesivamente en Kafka son los siguientes:\n",
    "\n",
    "- 101,Estacion Central,120\n",
    "- 102,Estacion Norte,90\n",
    "- 101,Estacion Central,130\n",
    "- 102,Estacion Norte,110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "065e98ac2bd3061d7706cd282041e722",
     "grade": true,
     "grade_id": "results-tests",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.2.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0839d46d-dc46-43dd-96f1-0a4addc45394;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 630ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0839d46d-dc46-43dd-96f1-0a4addc45394\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/9ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/11 17:53:25 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/11/11 17:53:25 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/11/11 17:53:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/11/11 17:53:25 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar added multiple times to distributed cache.\n",
      "25/11/11 17:53:27 WARN Client: Same path resource file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "bootstrap_servers = \"kafka:9092\"\n",
    "\n",
    "# Inicializar la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"CsvDataProcessing\").config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\").getOrCreate()\n",
    "\n",
    "csvStreamingDF = None # Sustituye con el código adecuado conforme a las instrucciones previas\n",
    "\n",
    "# Instrucciones:\n",
    "# 1. Uso de readStream en lugar de read:\n",
    "#    Utiliza readStream para crear un DataFrame que procese datos en tiempo real.\n",
    "# 2. Establecer el formato de origen a Kafka:\n",
    "#    Configura el DataFrame para leer datos desde Kafka.\n",
    "# 3. Configuración de los brokers de Kafka y el puerto:\n",
    "#    Especifica los nodos de Kafka y el puerto para la conexión.\n",
    "# 4. Suscripción al tópico \"datosCSV\":\n",
    "#    Define el tópico de Kafka al que debe suscribirse el DataFrame.\n",
    "# 5. Cargar la configuración con load():\n",
    "#    Inicia la lectura y procesamiento de los datos.\n",
    "\n",
    "# Descomenta las siguientes lineas y verás que debes completar el código \" <COMPLETAR> \"\n",
    "# csvStreamingDF = spark.readStream\\\n",
    "#     .format(<COMPLETAR>)\\\n",
    "#     .option(<COMPLETAR>, <COMPLETAR>)\\\n",
    "#     .option(<COMPLETAR>, <COMPLETAR>)\\\n",
    "#     .<COMPLETAR>()\n",
    "\n",
    "\n",
    "# Tu código hazlo aquí\n",
    "csvStreamingDF = spark.readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\\\n",
    "    .option(\"subscribe\", \"datosCSV\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones:\n",
    "1. Dividir cada línea por comas para obtener campos individuales.\n",
    " - Usa la función `split()` para dividir la cadena `value` en cada `coma`. Esto te dará un array de valores que lo almacenarás/asignarás a `fields`.\n",
    "2. Utiliza withColumn para crear tres nuevas columnas: 'id_estacion', 'nombre_estacion' y 'numero_pasajeros'.\n",
    " - 'id_estacion' será un entero, obtenido del primer elemento tras dividir 'value'.\n",
    " - 'nombre_estacion' será una cadena, obtenida del segundo elemento.\n",
    " - 'numero_pasajeros' será un entero, obtenido del tercer elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convertir los datos binarios a texto\n",
    "csvStreamingDF = csvStreamingDF.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "fields = None   # Elimina y descomenta las siguientes lineas y verás que debes completar el código \" <COMPLETAR> \"\n",
    "\n",
    "fields =  fields = split(col(\"value\"), \",\")\n",
    "csvStreamingDF = csvStreamingDF\\\n",
    "    .withColumn(\"id_estacion\", fields.getItem(0).cast(\"integer\"))\\\n",
    "    .withColumn(\"nombre_estacion\", fields.getItem(1))\\\n",
    "    .withColumn(\"numero_pasajeros\", fields.getItem(2).cast(\"integer\"))\n",
    "\n",
    "# Tu código hazlo aquí\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones:\n",
    "    1. Realiza una agregación sobre 'csvStreamingDF' para calcular el número total de pasajeros por estación.\n",
    "    2. Usa groupBy sobre la columna 'nombre_estacion'.\n",
    "    3. Calcula la suma de 'numero_pasajeros' y nómbrala como 'total_pasajeros'.\n",
    "    4. Este proceso creará un nuevo DataFrame 'aggregatedDF'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Ejemplo de código a completar:\n",
    "# Calcular el número total de pasajeros por estación\n",
    "aggregatedDF = csvStreamingDF.groupBy(\"nombre_estacion\").agg(F.sum(\"numero_pasajeros\").alias(\"total_pasajeros\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 17:54:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/11 17:54:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/11/11 17:54:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Configura la salida del stream para guardar los resultados en una tabla en memoria\n",
    "query = aggregatedDF.writeStream\\\n",
    "                    .queryName(\"agregadosPasajeros\")\\\n",
    "                    .outputMode(\"complete\")\\\n",
    "                    .format(\"memory\")\\\n",
    "                    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de completar la ejecución de la celda previa, inicia sesión en el productor de Kafka a través de SSH en una de las máquinas (puedes revisar las instrucciones de la práctica para recordar cómo hacerlo). Luego, inserta exactamente estos 4 mensajes a Kafka en formato CSV. Observarás que incluyen un campo `id_estacion`, `nombre_estacion` y un campo `numero_pasajeros`, representando los datos que recibiríamos en vivo de diferentes estaciones de bus conforme los trayectos van llegando a destino.\n",
    "\n",
    "\n",
    "### Instrucciones:\n",
    "    1. Utiliza spark.sql para ejecutar una consulta SQL que recupere todos los datos de la tabla 'agregadosPasajeros'.\n",
    "    2. Almacena el resultado de la consulta en el DataFrame 'retrasosAggregadosDF'.\n",
    "    3. Utiliza el método .show() para mostrar los resultados de la consulta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|nombre_estacion|total_pasajeros|\n",
      "+---------------+---------------+\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrasosAggregadosDF = None   # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "\n",
    "retrasosAggregadosDF = spark.sql(\"select * from agregadosPasajeros\")\n",
    "retrasosAggregadosDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====>                                                 (94 + 1) / 1000]\r"
     ]
    }
   ],
   "source": [
    "# Verifica que las columnas sean correctas\n",
    "columnas = retrasosAggregadosDF.columns\n",
    "assert(len(columnas) == 2)\n",
    "assert(\"nombre_estacion\" in columnas)\n",
    "assert(\"total_pasajeros\" in columnas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones:\n",
    "    1. Inicializa una variable para almacenar el total de pasajeros para la \"Estacion Central\".\n",
    "    2. Usa el DataFrame 'retrasosAggregadosDF' para filtrar los datos de la \"Estacion Central\".\n",
    "    3. Selecciona la columna 'total_pasajeros' y usa .collect() para obtener el valor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "| nombre_estacion|total_pasajeros|\n",
      "+----------------+---------------+\n",
      "|Estacion Central|            120|\n",
      "+----------------+---------------+\n",
      "\n",
      "120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 18:08:40 WARN TaskSetManager: Lost task 585.0 in stage 13.0 (TID 2801) (uax-cluster-w-0.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 7): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/585/1.delta of HDFSStateStoreProvider[id = (op=0,part=585),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/585]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/585/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/585/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "25/11/11 18:08:40 WARN TaskSetManager: Lost task 582.0 in stage 13.0 (TID 2800) (uax-cluster-w-0.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 10): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/582/1.delta of HDFSStateStoreProvider[id = (op=0,part=582),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/582]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/582/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/582/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "25/11/11 18:08:44 WARN TaskSetManager: Lost task 803.0 in stage 13.0 (TID 2913) (uax-cluster-w-0.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 7): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/803/1.delta of HDFSStateStoreProvider[id = (op=0,part=803),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/803]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/803/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/803/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "25/11/11 18:08:44 WARN TaskSetManager: Lost task 804.0 in stage 13.0 (TID 2914) (uax-cluster-w-0.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 10): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/804/1.delta of HDFSStateStoreProvider[id = (op=0,part=804),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/804]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/804/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/804/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Mostrar los resultados de la consulta\n",
    "retrasosAggregadosDF.show()\n",
    "total_pasajeros_estacion_central_primer_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "\n",
    "\n",
    "total_pasajeros_estacion_central_primer_mensaje = retrasosAggregadosDF.where(F.col(\"nombre_estacion\") == \"Estacion Central\")\\\n",
    "                                               .select(F.col(\"total_pasajeros\"))\\\n",
    "                                               .collect()[0]['total_pasajeros']\n",
    "print(total_pasajeros_estacion_central_primer_mensaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones:\n",
    "    1. Inicializa variables para almacenar el total de pasajeros para las estaciones \"Estacion Central\" y \"Estacion Norte\".\n",
    "    2. Utiliza el DataFrame 'retrasosAggregadosDF' para filtrar los datos de cada estación.\n",
    "    3. Selecciona la columna 'total_pasajeros' y usa .collect() para obtener los valores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "| nombre_estacion|total_pasajeros|\n",
      "+----------------+---------------+\n",
      "|  Estacion Norte|             90|\n",
      "|Estacion Central|            120|\n",
      "+----------------+---------------+\n",
      "\n",
      "120\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 18:11:16 WARN TaskSetManager: Lost task 494.0 in stage 19.0 (TID 3783) (uax-cluster-w-1.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 11): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/494/1.delta of HDFSStateStoreProvider[id = (op=0,part=494),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/494]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/494/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/494/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "25/11/11 18:11:16 WARN TaskSetManager: Lost task 498.0 in stage 19.0 (TID 3784) (uax-cluster-w-1.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 8): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/498/1.delta of HDFSStateStoreProvider[id = (op=0,part=498),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/498]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/498/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/498/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "25/11/11 18:11:20 WARN TaskSetManager: Lost task 749.0 in stage 19.0 (TID 3913) (uax-cluster-w-1.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 8): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/749/1.delta of HDFSStateStoreProvider[id = (op=0,part=749),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/749]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/749/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/749/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "25/11/11 18:11:20 WARN TaskSetManager: Lost task 752.0 in stage 19.0 (TID 3914) (uax-cluster-w-1.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 11): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/752/1.delta of HDFSStateStoreProvider[id = (op=0,part=752),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/752]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/752/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/752/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "25/11/11 18:11:24 WARN TaskSetManager: Lost task 976.0 in stage 19.0 (TID 4021) (uax-cluster-w-1.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 8): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/976/1.delta of HDFSStateStoreProvider[id = (op=0,part=976),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/976]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/976/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/976/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "25/11/11 18:11:24 WARN TaskSetManager: Lost task 973.0 in stage 19.0 (TID 4020) (uax-cluster-w-1.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 11): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/973/1.delta of HDFSStateStoreProvider[id = (op=0,part=973),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/973]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/973/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/973/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "25/11/11 18:11:24 WARN TaskSetManager: Lost task 977.0 in stage 19.0 (TID 4022) (uax-cluster-w-1.europe-southwest1-a.c.tough-zoo-473614-g1.internal executor 8): java.lang.IllegalStateException: Error reading delta file file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/977/1.delta of HDFSStateStoreProvider[id = (op=0,part=977),dir = file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/977]: file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/977/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:464)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:420)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:419)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:554)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:391)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getReadStore(HDFSBackedStateStoreProvider.scala:215)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getReadOnly(StateStore.scala:492)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.ReadStateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:123)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-e96e87ce-1561-4e53-8f2b-f1ba3ffe3703/state/0/977/1.delta does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:915)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1236)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:905)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:392)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:192)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:724)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:227)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:886)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:882)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:888)\n",
      "\tat org.apache.spark.sql.execution.streaming.AbstractFileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:323)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:460)\n",
      "\t... 32 more\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Mostrar los resultados de la consulta\n",
    "retrasosAggregadosDF.show()\n",
    "total_pasajeros_estacion_central_segundo_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "total_pasajeros_estacion_norte_segundo_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "\n",
    "total_pasajeros_estacion_central_segundo_mensaje = retrasosAggregadosDF.where(col(\"nombre_estacion\") == \"Estacion Central\")\\\n",
    "                                                        .select(col(\"total_pasajeros\"))\\\n",
    "                                                        .collect()[0]['total_pasajeros']\n",
    "\n",
    "total_pasajeros_estacion_norte_segundo_mensaje = retrasosAggregadosDF.where(col(\"nombre_estacion\") == \"Estacion Norte\")\\\n",
    "                                                      .select(col(\"total_pasajeros\"))\\\n",
    "                                                      .collect()[0]['total_pasajeros']\n",
    "\n",
    "print(total_pasajeros_estacion_central_segundo_mensaje)\n",
    "print(total_pasajeros_estacion_norte_segundo_mensaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "| nombre_estacion|total_pasajeros|\n",
      "+----------------+---------------+\n",
      "|  Estacion Norte|             90|\n",
      "|Estacion Central|            250|\n",
      "+----------------+---------------+\n",
      "\n",
      "250\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los resultados de la consulta\n",
    "retrasosAggregadosDF.show()\n",
    "total_pasajeros_estacion_central_tercero_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "total_pasajeros_estacion_norte_tercero_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "total_pasajeros_estacion_sur_tercero_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session \n",
    "\n",
    "total_pasajeros_estacion_central_tercero_mensaje = retrasosAggregadosDF.where(col(\"nombre_estacion\") == \"Estacion Central\")\\\n",
    "                                                        .select(col(\"total_pasajeros\"))\\\n",
    "                                                        .collect()[0]['total_pasajeros']\n",
    "\n",
    "total_pasajeros_estacion_norte_tercero_mensaje = retrasosAggregadosDF.where(col(\"nombre_estacion\") == \"Estacion Norte\")\\\n",
    "                                                      .select(col(\"total_pasajeros\"))\\\n",
    "                                                      .collect()[0]['total_pasajeros']\n",
    "\n",
    "#total_pasajeros_estacion_sur_tercero_mensaje = retrasosAggregadosDF.where(col(\"nombre_estacion\") == \"Estacion Sur\")\\\n",
    "#                                                      .select(col(\"total_pasajeros\"))\\\n",
    "#                                                      .collect()[0]['total_pasajeros']\n",
    "\n",
    "print(total_pasajeros_estacion_central_tercero_mensaje)\n",
    "print(total_pasajeros_estacion_norte_tercero_mensaje)\n",
    "#print(total_pasajeros_estacion_sur_tercero_mensaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "| nombre_estacion|total_pasajeros|\n",
      "+----------------+---------------+\n",
      "|  Estacion Norte|            200|\n",
      "|Estacion Central|            250|\n",
      "+----------------+---------------+\n",
      "\n",
      "250\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los resultados de la consulta\n",
    "retrasosAggregadosDF.show()\n",
    "total_pasajeros_estacion_central_quarto_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "total_pasajeros_estacion_norte_quarto_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "total_pasajeros_estacion_sur_quarto_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session \n",
    "otal_pasajeros_estacion_este_quarto_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session \n",
    "\n",
    "total_pasajeros_estacion_central_quarto_mensaje = retrasosAggregadosDF.where(col(\"nombre_estacion\") == \"Estacion Central\")\\\n",
    "                                                        .select(col(\"total_pasajeros\"))\\\n",
    "                                                        .collect()[0]['total_pasajeros']\n",
    "\n",
    "total_pasajeros_estacion_norte_quarto_mensaje = retrasosAggregadosDF.where(col(\"nombre_estacion\") == \"Estacion Norte\")\\\n",
    "                                                      .select(col(\"total_pasajeros\"))\\\n",
    "                                                      .collect()[0]['total_pasajeros']\n",
    "\n",
    "# total_pasajeros_estacion_sur_quarto_mensaje = retrasosAggregadosDF.where(<COMPLETAR>(\"nombre_estacion\") == \"Estacion Sur\")\\\n",
    "#                                                       .<COMPLETAR>(<COMPLETAR>(\"total_pasajeros\"))\\\n",
    "#                                                       .<COMPLETAR>()[0]['total_pasajeros']\n",
    "\n",
    "# total_pasajeros_estacion_este_quarto_mensaje = retrasosAggregadosDF.where(<COMPLETAR>(\"nombre_estacion\") == \"Estacion Este\")\\\n",
    "#                                                       .<COMPLETAR>(<COMPLETAR>(\"total_pasajeros\"))\\\n",
    "#                                                       .<COMPLETAR>()[0]['total_pasajeros']\n",
    "print(total_pasajeros_estacion_central_quarto_mensaje)\n",
    "print(total_pasajeros_estacion_norte_quarto_mensaje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insertar en el quinto mensaje la Estacion Central de nuevo para comprobar que realiza correctamente la suma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los resultados de la consulta\n",
    "retrasosAggregadosDF.show()\n",
    "total_pasajeros_estacion_central_quinto_mensaje = None # Cambia esta linea para llamaar al método .sql de la sesión de Spark session\n",
    "\n",
    "# total_pasajeros_estacion_central_quinto_mensaje = retrasosAggregadosDF.where(<COMPLETAR>(\"nombre_estacion\") == \"Estacion Central\")\\\n",
    "#                                                         .<COMPLETAR>(<COMPLETAR>(\"total_pasajeros\"))\\\n",
    "#                                                         .<COMPLETAR>()[0]['total_pasajeros']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aserción para verificar que el total de pasajeros para 'Estacion Central' es 240\n",
    "assert total_pasajeros_estacion_central_quinto_mensaje == 240, \"El total de pasajeros para Estacion Central debería ser 240\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
